{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs4\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "import getpass\n",
    "import easygui as eg\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def click(x):\n",
    "    button = driver.find_element(By.XPATH, x)\n",
    "    button.click()\n",
    "    time.sleep(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scraping from stepstone"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scraping from stepstone.de to get page source of the job results pages (page 1, page 2, page 3, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Downloading: 100%|██████████| 6.79M/6.79M [00:01<00:00, 6.74MB/s]\n",
      "C:\\Users\\mdima\\AppData\\Local\\Temp\\ipykernel_32596\\2352680371.py:10: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(ChromeDriverManager().install())\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.stepstone.de/jobs/python-data-analyst\"\n",
    "\n",
    "options = Options()\n",
    "# options.add_argument(\"--disable-notifications\")\n",
    "# to prevent from being spotted as a robot\n",
    "options.add_argument('--disable-gpu')\n",
    "options.add_argument('user-agent=fake-useragent')\n",
    "# installing chromedriver, so that we dont need to keep the chromedriver file\n",
    "# that needs to be updated every once in a while. better install the latest automatically\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "# opens the browser, maximize window size\n",
    "# but its often problematic if run in mac. so you can comment this one out\n",
    "# driver.maximize_window()\n",
    "# opening url\n",
    "driver.get(url)\n",
    "time.sleep(6)\n",
    "ok1 = \"/html/body/div[10]/section/div/section/div[2]/div[1]/div[2]/div\"\n",
    "ok2 = \"/html/body/div[9]/section/div/section/div[2]/div[1]/div[2]/div\"\n",
    "ok3 = \"/html/body/div[10]/section/div/section/div[2]/div[1]/div[2]\"\n",
    "try:\n",
    "    click(ok1)\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    click(ok2)\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    click(ok3)\n",
    "except:\n",
    "    pass\n",
    "npage = \"/html/body/div[4]/div[1]/div/div/div[2]/div/div[2]/div[3]/div/nav/ul/li[9]/a\"\n",
    "html1 = driver.page_source\n",
    "search_result_1 = []\n",
    "end = False\n",
    "while not end:\n",
    "    search_result_1.append(html1)\n",
    "    time.sleep(1)\n",
    "    i = 0\n",
    "    while i < 4:\n",
    "        driver.execute_script(\"window.scrollTo(0, window.scrollY + 1500)\") \n",
    "        time.sleep(.2)\n",
    "        i+=1\n",
    "    time.sleep(4)\n",
    "    click(npage)\n",
    "    html2 = driver.page_source\n",
    "    # comparing html_before and html_after\n",
    "    if html1 == html2:\n",
    "        end = True\n",
    "    else:\n",
    "        html1 = html2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 337/337 [00:41<00:00,  8.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8405\n",
      "(8405, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.stepstone.de/stellenangebote--Prod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.stepstone.de/stellenangebote--Data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.stepstone.de/stellenangebote--Data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.stepstone.de/stellenangebote--Data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.stepstone.de/stellenangebote--Data...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             job_url\n",
       "0  https://www.stepstone.de/stellenangebote--Prod...\n",
       "1  https://www.stepstone.de/stellenangebote--Data...\n",
       "2  https://www.stepstone.de/stellenangebote--Data...\n",
       "3  https://www.stepstone.de/stellenangebote--Data...\n",
       "4  https://www.stepstone.de/stellenangebote--Data..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_url_1 = []\n",
    "for h in tqdm(search_result_1):\n",
    "    soup = bs4(h)\n",
    "    res = soup.find_all(\"article\",{\"class\":\"resultlist-1jx3vjx\"})\n",
    "    for r in res:\n",
    "        href = r.find(\"a\",{\"class\":\"resultlist-1uvdp0v\"}).get(\"href\")\n",
    "        href = \"https://www.stepstone.de\"+href\n",
    "        job_url_1.append(href)\n",
    "print(len(job_url_1))\n",
    "df_1 = pd.DataFrame(columns = [\"job_url\"])\n",
    "df_1[\"job_url\"] = job_url_1\n",
    "print(df_1.shape)\n",
    "df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.to_csv(\"job url 1.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv(\"job url 1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the csv file above into smaller csv files\n",
    "def split_file(df):\n",
    "    fileindex = 1\n",
    "    for i in range(0,8501,500):\n",
    "        a = df.iloc[i:i+500] # each smaller file contains 500 rows\n",
    "        filename = str(fileindex)+\".csv\"\n",
    "        a.to_csv(filename, index = False)\n",
    "        fileindex += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i needed to split the file so other students could help me with scraping. the time was too short to scrape everything by myself\n",
    "split_file(df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function reads different csv files containing page urls. \n",
    "# then get only the page source. then create csv files for each corresponding url files\n",
    "# it will be concatenated later on\n",
    "def get_page_source(a,b):\n",
    "    for f in tqdm(range(a,b)): # 17 is the number of split files\n",
    "        source = []\n",
    "        filename = str(f)+\".csv\" # defining file name\n",
    "        df_url = pd.read_csv(\"./\"+filename)\n",
    "        job_url = df_url[\"job_url\"].tolist()\n",
    "        options = Options()\n",
    "        # installing chromedriver, so that we dont need to keep the chromedriver file\n",
    "        # that needs to be updated every once in a while. better install the latest automatically\n",
    "        driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "        for i in job_url: # get full page source for each job offer page\n",
    "            # options.add_argument(\"--disable-notifications\")\n",
    "            # to prevent from being spotted as a robot\n",
    "            options.add_argument('--disable-gpu')\n",
    "            options.add_argument('user-agent=fake-useragent')\n",
    "            # opens the browser, maximize window size\n",
    "            # opening url\n",
    "            driver.get(i)\n",
    "            time.sleep(3.5)\n",
    "            page = driver.page_source\n",
    "            source.append(page)\n",
    "        # saving source column separately for each splitted file\n",
    "        job_source = pd.DataFrame(columns = [\"job_source\"])\n",
    "        job_source[\"job_source\"] = source\n",
    "        filename = str(f)+\" source.csv\"\n",
    "        job_source.to_csv(\"./\"+filename, index = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DONT RUN THIS: TOO HEAVY AND TAKES TOO MUCH TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put a pair of number from 0 to 18 as parameters. \n",
    "# if you want to do all in one go, then put 0,18\n",
    "# if you want to split the job, put 0,3 then 3,5 then 5,8 and so on\n",
    "# the max number was set to 18. you can change it in the previous cell\n",
    "# successfully scraped until: 4\n",
    "get_page_source(5,7)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "getting some details from each page source we had from the big scraping job (see cell above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [34:16<00:00, 186.93s/it]\n"
     ]
    }
   ],
   "source": [
    "df_stepstone = pd.DataFrame()\n",
    "url_1,d_1,t_1,cn_1,cl_1,c_1= [],[],[],[],[],[]\n",
    "files = [1,3,4,5,7,8,9,10,12,13,14]\n",
    "for f in tqdm(files):\n",
    "    filename1 = str(f)+\".csv\"\n",
    "    filename2 = str(f)+\" source.csv\"\n",
    "    df1 = pd.read_csv(\"./\"+filename1)\n",
    "    df2 = pd.read_csv(\"./\"+filename2)\n",
    "    description_1,title_1,comp_name_1,comp_url_1,city_1 = [],[],[],[],[]\n",
    "    source = df2[\"source\"].tolist()\n",
    "    for s in source:\n",
    "        soup = bs4(s, \"html.parser\")\n",
    "        try:\n",
    "            jobtitle = soup.find(\"span\",{\"data-at\":\"header-job-title\"}).text\n",
    "        except:\n",
    "            jobtitle = \"unknown\"\n",
    "        title_1.append(jobtitle)\n",
    "        try:\n",
    "            compname = soup.find(\"a\",{\"data-at\":\"header-company-name\"}).text\n",
    "        except:\n",
    "            compname = \"unknown\"\n",
    "        comp_name_1.append(compname)\n",
    "        try:\n",
    "            complink = soup.find(\"a\",{\"data-at\":\"header-company-name\"}).get(\"href\")\n",
    "        except:\n",
    "            complink = \"unknown\"\n",
    "        comp_url_1.append(complink)\n",
    "        try:\n",
    "            city = soup.find(\"span\",{\"class\":\"listing-content-provider-1u79rpn\"}).text\n",
    "        except:\n",
    "            city = \"unknown\"\n",
    "        city_1.append(city)\n",
    "        infotext = soup.find_all(\"div\",{\"class\":\"listing-content-provider-10ltcrf\"})\n",
    "        desc = []\n",
    "        for i in infotext:\n",
    "            try:\n",
    "                texts = i.find_all(\"p\")\n",
    "                for t in texts:\n",
    "                    info = t.text\n",
    "                    desc.append(info)\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                texts = i.find_all(\"li\")\n",
    "                for t in texts:\n",
    "                    info = t.text\n",
    "                    desc.append(info)\n",
    "            except:\n",
    "                pass\n",
    "        # enemy spotted\n",
    "        description = \" \".join(desc).replace(\"\\xa0\",\"\").replace(\"\\\\n\",\"\")\n",
    "        description_1.append(description)\n",
    "    d_1.extend(description_1)\n",
    "    t_1.extend(title_1)\n",
    "    cn_1.extend(comp_name_1)\n",
    "    cl_1.extend(comp_url_1)\n",
    "    c_1.extend(city_1)\n",
    "    url_1.extend(df1[\"href_to_scrape\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5500"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 0\n",
    "for i in t_1:\n",
    "    if i != None:\n",
    "        x+=1\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5500, 6)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stepstone[\"job_url\"] = url_1\n",
    "df_stepstone[\"description\"] = d_1\n",
    "df_stepstone[\"job_title\"] = t_1\n",
    "df_stepstone[\"comp_name\"] = cn_1\n",
    "df_stepstone[\"comp_link\"] = cl_1\n",
    "df_stepstone[\"city\"] = c_1\n",
    "df_stepstone.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5500, 6)\n",
      "(4780, 6)\n"
     ]
    }
   ],
   "source": [
    "for i in df_stepstone.columns:\n",
    "    df_stepstone[i] = df_stepstone[i].replace(\"unknown\", np.nan)\n",
    "print(df_stepstone.shape)\n",
    "df_stepstone = df_stepstone.dropna(subset = \"city\")\n",
    "print(df_stepstone.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stepstone.to_csv(\"stepstone 1 incomplete.csv\", index = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now scrape from indeed.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url2 = \"https://de.indeed.com/jobs?q=python+data+analyst&l=&from=searchOnHP&vjk=79940d31c430a131\"\n",
    "options = Options()\n",
    "# options.add_argument(\"--disable-notifications\")\n",
    "# to prevent from being spotted as a robot\n",
    "options.add_argument('--disable-gpu')\n",
    "options.add_argument('user-agent=fake-useragent')\n",
    "# installing chromedriver, so that we dont need to keep the chromedriver file\n",
    "# that needs to be updated every once in a while. better install the latest automatically\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "# opens the browser, maximize window size\n",
    "driver.maximize_window()\n",
    "# opening url\n",
    "driver.get(url2)\n",
    "time.sleep(2)\n",
    "html1 = driver.page_source\n",
    "npage = \"/html/body/main/div/div[1]/div/div/div[5]/div[1]/nav/div[6]/a\"\n",
    "lhtml2 = []\n",
    "end = False\n",
    "counter = 1\n",
    "while not end:\n",
    "    if counter == 2:\n",
    "        time.sleep(2)\n",
    "        webdriver.ActionChains(driver).send_keys(Keys.ESCAPE).perform()\n",
    "    lhtml2.append(html1)\n",
    "    time.sleep(1)\n",
    "    i = 0\n",
    "    while i < 4:\n",
    "        driver.execute_script(\"window.scrollTo(0, window.scrollY + 1500)\") \n",
    "        time.sleep(.2)\n",
    "        i+=1\n",
    "    time.sleep(1)\n",
    "    try:\n",
    "        click(npage)\n",
    "    except:\n",
    "        end = True\n",
    "    counter += 1\n",
    "    html2 = driver.page_source\n",
    "    # comparing html_before and html_after\n",
    "    if html1 == html2:\n",
    "        end = True\n",
    "    else:\n",
    "        html1 = html2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lhref2 = []\n",
    "for h in tqdm(lhtml2):\n",
    "    soup = bs4(h)\n",
    "    container = soup.find(\"ul\",{\"class\":\"jobsearch-ResultsList css-0\"})\n",
    "    res = container.find_all(\"div\",{\"class\":\"slider_container css-g7s71f eu4oa1w0\"})\n",
    "    for r in res:\n",
    "        href = r.find(\"h2\",{\"tabindex\":\"-1\"}).find(\"a\").get(\"href\")\n",
    "        href = \"https://de.indeed.com\" + href\n",
    "        lhref2.append(href)\n",
    "print(len(lhref2))\n",
    "href_pd2 = pd.DataFrame(columns = [\"job_url\"])\n",
    "href_pd2[\"job_url\"] = lhref2\n",
    "# href_pd2.to_csv(\"job url 2.csv\", index = False)\n",
    "print(href_pd2.shape)\n",
    "href_pd2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "href_pd2 = pd.read_csv(\"job url 2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source2 = []\n",
    "options = Options()\n",
    "# installing chromedriver, so that we dont need to keep the chromedriver file\n",
    "# that needs to be updated every once in a while. better install the latest automatically\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "for i in tqdm(lhref2):\n",
    "    # options.add_argument(\"--disable-notifications\")\n",
    "    # to prevent from being spotted as a robot\n",
    "    options.add_argument('--disable-gpu')\n",
    "    options.add_argument('user-agent=fake-useragent')\n",
    "    # opens the browser, maximize window size\n",
    "    driver.maximize_window()\n",
    "    # opening url\n",
    "    driver.get(i)\n",
    "    time.sleep(1)\n",
    "    page = driver.page_source\n",
    "    source2.append(page)\n",
    "href_pd2[\"job_source\"] = source2\n",
    "# href_pd2.to_csv(\"job url 2.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "href_pd2 = pd.read_csv(\"job url 2.csv\")\n",
    "href_pd2.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldescription2,ltitle2,lcompname2,lcomplink2,lcity2 = [],[],[],[],[]\n",
    "\n",
    "for s in tqdm(source2):\n",
    "    soup = bs4(s, \"html.parser\")\n",
    "    try:\n",
    "        jobtitle = soup.find(\"h1\",{\"class\":\"icl-u-xs-mb--xs icl-u-xs-mt--none jobsearch-JobInfoHeader-title\"}).text\n",
    "    except:\n",
    "        jobtitle = None\n",
    "    ltitle2.append(jobtitle)\n",
    "    try:\n",
    "        compname = soup.find(\"div\",{\"data-company-name\":\"true\"}).text\n",
    "    except:\n",
    "        compname = None\n",
    "    lcompname2.append(compname)\n",
    "    try:\n",
    "        complink = soup.find(\"div\",{\"data-company-name\":\"true\"}).find(\"a\").get(\"href\")\n",
    "    except:\n",
    "        complink = None\n",
    "    lcomplink2.append(complink)\n",
    "    city = None\n",
    "    lcity2.append(city)\n",
    "    infotext = soup.find(\"div\",{\"id\":\"jobDescriptionText\"}).find_all(\"p\")\n",
    "    desc = []\n",
    "    for i in infotext:\n",
    "        try:\n",
    "            texts = i.find(\"b\").text\n",
    "            desc.append(texts)\n",
    "        except:\n",
    "            texts = i.text\n",
    "        desc.append(texts)\n",
    "    # enemy spotted\n",
    "    description = \" \".join(desc)\n",
    "    ldescription2.append(description)\n",
    "print(len(ldescription2), len(source2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "href_pd2[\"description\"] = ldescription2\n",
    "href_pd2[\"job_title\"] = ltitle2\n",
    "href_pd2[\"comp_name\"] = lcompname2\n",
    "href_pd2[\"comp_link\"] = lcomplink2\n",
    "href_pd2[\"city\"] = lcity2\n",
    "# href_pd2.to_csv(\"job url 2.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "href_pd2 = pd.read_csv(\"job url 2.csv\") # sorry about the filename. i will change it to a better one if i got time\n",
    "href_pd2.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = pd.concat([href_pd, href_pd2], axis = 0)\n",
    "# full.to_csv(\"full.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1acc24bd1901f9ae8c29efb6830fcc1ca9fe0219dd00f8f1dc1b91856def15a9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
